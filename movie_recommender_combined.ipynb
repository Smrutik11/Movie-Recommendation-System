{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Movie Recommendation System \u2014 Combined Notebook\n\nThis single notebook includes EDA, user/item collaborative filtering, matrix factorization (SVD), evaluation, and instructions to save and deploy a demo. Follow cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup & Notes\n\n- Place MovieLens 100k dataset under `data/ml-100k/` (required files: `u.data`, `u.item`).\n- Install requirements: `pip install -r requirements.txt` (file included in project zip)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# 0.1 Imports\nimport os\nimport math\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom surprise import Dataset, Reader, SVD, accuracy\nfrom surprise.model_selection import train_test_split as svd_train_test_split\n\n# Display settings\npd.set_option('display.max_columns', 50)\npd.set_option('display.width', 120)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n\nLoad `u.data` and `u.item`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nDATA_DIR = 'data/ml-100k'  # adjust if needed\nif not os.path.exists(DATA_DIR):\n    print(\"Warning: data/ml-100k not found. Please download MovieLens 100k and place it under data/ml-100k/\")\ncols = ['user_id','movie_id','rating','timestamp']\nratings = pd.read_csv(os.path.join(DATA_DIR,'u.data'), sep='\\t', names=cols, encoding='latin-1')\nmovie_cols = ['movie_id','title','release_date','video_release_date','IMDb_URL',\n              'unknown','Action','Adventure','Animation','Children','Comedy','Crime','Documentary',\n              'Drama','Fantasy','Film-Noir','Horror','Musical','Mystery','Romance','Sci-Fi',\n              'Thriller','War','Western']\nmovies = pd.read_csv(os.path.join(DATA_DIR,'u.item'), sep='|', names=movie_cols, encoding='latin-1', header=None)\nprint('Ratings rows:', len(ratings))\nratings.head()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)\n\nSimple statistics and plots: rating distribution, users, movies, sparsity, popular movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Basic stats\nnum_ratings = len(ratings)\nnum_users = ratings['user_id'].nunique()\nnum_movies = ratings['movie_id'].nunique()\nsparsity = 1.0 - num_ratings / (num_users * num_movies)\nprint(f\"Ratings: {num_ratings}, Users: {num_users}, Movies: {num_movies}, Sparsity: {sparsity:.4f}\")\n\n# Rating distribution\nplt.figure(figsize=(6,4))\nratings['rating'].hist(bins=5)\nplt.title('Rating distribution')\nplt.xlabel('Rating')\nplt.show()\n\n# Top 10 most-rated movies\npop = ratings.groupby('movie_id').size().reset_index(name='count').sort_values('count', ascending=False)\ntop10 = pop.head(10).merge(movies[['movie_id','title']], on='movie_id')\ntop10[['title','count']]\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Prepare training and test sets\n\nWe split the raw ratings dataframe so evaluation is done on unseen user-item pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\ntrain_df, test_df = train_test_split(ratings, test_size=0.2, random_state=42)\nprint('Train size:', len(train_df), 'Test size:', len(test_df))\n# Pivot (for algorithms that require full matrix)\ntrain_matrix = train_df.pivot(index='user_id', columns='movie_id', values='rating').fillna(0)\ntrain_matrix.shape\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. User-based Collaborative Filtering\n\nCompute cosine similarity between users and predict using k nearest neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# 4.1 Compute user-user cosine similarity\nuser_ids = train_matrix.index.tolist()\nuser_matrix = train_matrix.values  # rows: users, cols: movies\nuser_sim = cosine_similarity(user_matrix)\nprint('User similarity matrix shape:', user_sim.shape)\n\n# 4.2 Helper functions\nuser_id_to_index = {uid: i for i, uid in enumerate(user_ids)}\nindex_to_user_id = {i: uid for uid, i in user_id_to_index.items()}\n\ndef predict_user_based(user_id, movie_id, k=20):\n    # fallback to global mean\n    global_mean = train_df['rating'].mean()\n    if user_id not in user_id_to_index:\n        return global_mean\n    if movie_id not in train_matrix.columns:\n        return global_mean\n    uidx = user_id_to_index[user_id]\n    movie_col_idx = list(train_matrix.columns).index(movie_id)\n    sims = user_sim[uidx]\n    other_ratings = user_matrix[:, movie_col_idx]\n    pairs = [(sims[i], other_ratings[i]) for i in range(len(sims)) if other_ratings[i] > 0 and i != uidx]\n    if not pairs:\n        return global_mean\n    pairs.sort(key=lambda x: x[0], reverse=True)\n    topk = pairs[:k]\n    num = sum(sim * rating for sim, rating in topk)\n    den = sum(abs(sim) for sim, _ in topk)\n    if den == 0:\n        return global_mean\n    return num / den\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Evaluate user-based CF on sample of test set (for speed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\ndef evaluate_preds(preds, trues):\n    preds = np.array(preds)\n    trues = np.array(trues)\n    rmse = np.sqrt(np.mean((preds - trues)**2))\n    mask = trues != 0\n    mape = np.mean(np.abs((preds[mask] - trues[mask]) / trues[mask])) * 100\n    return rmse, mape\n\nsample_test = test_df.sample(n=2000, random_state=42) if len(test_df) > 2000 else test_df\npreds, trues = [], []\nfor _, row in sample_test.iterrows():\n    p = predict_user_based(int(row['user_id']), int(row['movie_id']), k=30)\n    preds.append(p); trues.append(row['rating'])\nrmse_u, mape_u = evaluate_preds(preds, trues)\nprint('User-based CF RMSE:', rmse_u, 'MAPE:', mape_u)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Item-based Collaborative Filtering\n\nBuild item vectors and use cosine similarity between items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Item matrix (movies x users)\nitem_matrix = train_matrix.T\nitem_ids = item_matrix.index.tolist()\nitem_matrix_vals = item_matrix.values\nitem_sim = cosine_similarity(item_matrix_vals)\nitem_id_to_index = {iid: i for i, iid in enumerate(item_ids)}\n\ndef predict_item_based(user_id, movie_id, k=20):\n    global_mean = train_df['rating'].mean()\n    if user_id not in train_matrix.index or movie_id not in item_id_to_index:\n        return global_mean\n    user_ratings = train_matrix.loc[user_id]\n    item_idx = item_id_to_index[movie_id]\n    sims = item_sim[item_idx]\n    rated_items = [(sims[item_id_to_index[mid]], user_ratings[mid]) for mid in train_matrix.columns if user_ratings[mid] > 0 and mid != movie_id]\n    if not rated_items:\n        return global_mean\n    rated_items.sort(key=lambda x: x[0], reverse=True)\n    topk = rated_items[:k]\n    num = sum(sim * rating for sim, rating in topk)\n    den = sum(abs(sim) for sim, _ in topk)\n    if den == 0:\n        return global_mean\n    return num / den\n\n# Evaluate item-based\npreds, trues = [], []\nfor _, row in sample_test.iterrows():\n    p = predict_item_based(int(row['user_id']), int(row['movie_id']), k=30)\n    preds.append(p); trues.append(row['rating'])\nrmse_i, mape_i = evaluate_preds(preds, trues)\nprint('Item-based CF RMSE:', rmse_i, 'MAPE:', mape_i)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Matrix Factorization \u2014 SVD (Surprise)\n\nUse the Surprise library's SVD algorithm for a robust matrix factorization baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Prepare Surprise dataset\nreader = Reader(line_format='user item rating timestamp', sep='\\t')\ndata = Dataset.load_from_file(os.path.join(DATA_DIR,'u.data'), reader=reader)\ntrainset, testset = svd_train_test_split(data, test_size=0.2, random_state=42)\nalgo = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02, random_state=42)\nprint('Training SVD (this may take a moment)...')\nalgo.fit(trainset)\npredictions = algo.test(testset)\nrmse_svd = accuracy.rmse(predictions, verbose=True)\n# compute MAPE\ny_true = np.array([pred.r_ui for pred in predictions])\ny_pred = np.array([pred.est for pred in predictions])\nmape_svd = np.mean(np.abs((y_pred - y_true) / y_true)) * 100\nprint('SVD MAPE:', mape_svd)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Top-N Recommendations (SVD)\n\nGet top-N for a target user using the trained SVD model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\ndef get_unseen_items(train_df, user_id):\n    seen = set(train_df[train_df.user_id == user_id].movie_id)\n    all_items = set(train_df.movie_id.unique())\n    return list(all_items - seen)\n\ndef recommend_svd(algo, train_df, user_id, movies_df, topn=10):\n    unseen = get_unseen_items(train_df, user_id)\n    preds = []\n    for iid in unseen:\n        try:\n            est = algo.predict(user_id, iid).est\n        except Exception:\n            est = 0\n        preds.append((iid, est))\n    preds.sort(key=lambda x: x[1], reverse=True)\n    res = []\n    for mid, score in preds[:topn]:\n        title = movies_df[movies_df.movie_id == mid]['title'].values[0]\n        res.append((mid, title, score))\n    return res\n\n# Example recommendations for user 1\nprint('Top 10 recommendations for user 1:')\nrecos = recommend_svd(algo, train_df, 1, movies, topn=10)\nfor mid, title, score in recos:\n    print(f\"{title} (movie_id={mid}) \u2014 predicted {score:.2f}\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Ranking Metrics: Precision@K & Recall@K\n\nEvaluate top-K recommendation quality by treating ratings >= 4 as relevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\ndef precision_recall_at_k(predictions, k=10, threshold=4.0):\n    user_preds = defaultdict(list)\n    for uid, iid, true_r, est in predictions:\n        user_preds[uid].append((iid, est, true_r))\n    precisions = []\n    recalls = []\n    for uid, items in user_preds.items():\n        items.sort(key=lambda x: x[1], reverse=True)\n        topk = items[:k]\n        n_rel = sum(1 for _, _, true_r in items if true_r >= threshold)\n        n_rel_k = sum(1 for _, _, true_r in topk if true_r >= threshold)\n        precisions.append(n_rel_k / k if k > 0 else 0)\n        recalls.append(n_rel_k / n_rel if n_rel > 0 else 0)\n    return np.mean(precisions), np.mean(recalls)\n\n# Build a simple predictions list from Surprise predictions\nsurprise_preds = [(pred.uid, pred.iid, pred.r_ui, pred.est) for pred in predictions]\nprec, rec = precision_recall_at_k(surprise_preds, k=10, threshold=4.0)\nprint('Precision@10:', prec, 'Recall@10:', rec)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualizing item latent space (PCA)\n\nProject item factors to 2D to visualize clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Extract item factors from Surprise SVD (algo.qi is item factor matrix indexed by inner ids)\n# Need to map raw item ids to inner ids in trainset used by algo (trainset in Surprise)\ntry:\n    inner_id_map = algo.trainset._raw2inner_id_items\n    item_raw_ids = list(inner_id_map.keys())\n    item_latents = []\n    raw_ids = []\n    for raw_id, inner in inner_id_map.items():\n        item_latents.append(algo.qi[inner])\n        raw_ids.append(int(raw_id))\n    item_latents = np.array(item_latents)\n    pca = PCA(n_components=2)\n    coords = pca.fit_transform(item_latents)\n    plt.figure(figsize=(8,6))\n    plt.scatter(coords[:,0], coords[:,1], alpha=0.6, s=20)\n    plt.title('Item latent space (PCA projection)')\n    plt.xlabel('PC1'); plt.ylabel('PC2')\n    plt.show()\nexcept Exception as e:\n    print('Could not extract item latents:', e)\n    print('This may occur if Surprise used a different internal trainset than expected.')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save SVD model\n\nSave the trained SVD to `models/svd_model.pkl` so the Streamlit app can load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\nos.makedirs('models', exist_ok=True)\nwith open('models/svd_model.pkl', 'wb') as f:\n    pickle.dump(algo, f)\nprint('Saved SVD model to models/svd_model.pkl')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Streamlit demo (instructions)\n\nThe Streamlit demo `app/streamlit_app.py` in the repo loads `models/svd_model.pkl` and serves Top-N recommendations. Run:\n\n```bash\nstreamlit run app/streamlit_app.py\n```\n\nIf you wish, you can containerize the app with Docker or deploy on Streamlit Community Cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Next steps & improvements\n\n- Hyperparameter tuning (Surprise GridSearchCV)\n- Use implicit feedback and ALS for implicit signals\n- Hybrid recommender combining content (genres) and collaborative signals\n- Improve cold-start handling (content-based fallback)\n\n---\n\nThat's the combined notebook. Run cells sequentially. If anything errors due to missing data or packages, follow error messages to install packages or place the dataset."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}